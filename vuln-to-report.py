import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import defaultdict
from docx import Document

# Load vulnerability database from NVD API
cve_db = defaultdict(list)  # Dictionary to store CVE data
nvd_api_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"  # NIST API endpoint
params = {
    'resultsPerPage': 2000,  # Number of results to fetch per API call
    'startIndex': 0  # Starting index for pagination
}

def fetch_cve_data():
    """
    Fetch CVE (Common Vulnerabilities and Exposures) data from the NIST API.
    Populates the `cve_db` dictionary with CVE IDs, descriptions, and impact scores.
    """
    try:
        # Make a GET request to the NIST API
        response = requests.get(nvd_api_url, params=params)
        if response.status_code == 200:  # Check if the request was successful
            cve_data = response.json().get('vulnerabilities', [])  # Extract CVE data from the API response
            for cve in cve_data:
                cve_info = cve.get('cve', {})  # Extract CVE details
                cve_id = cve_info.get('id')  # Get the CVE ID
                description = cve_info.get('descriptions', [{}])[0].get('value', '')  # Get the CVE description
                impact = cve_info.get('metrics', {}).get('cvssMetricV31', [{}])[0].get('cvssData', {}).get('baseScore', 'N/A')  # Get the CVSS score
                # Store CVE data in the dictionary
                cve_db[cve_id].append({
                    'id': cve_id,
                    'summary': description,
                    'version': impact  # Using baseScore as a proxy for version/impact
                })
        else:
            print("Failed to fetch CVE data from NVD API.")  # Handle API request failure
    except requests.exceptions.RequestException as e:
        print(f"Error fetching CVE data: {e}")  # Handle exceptions during the API request

# Fetch CVE data when the script starts
fetch_cve_data()

def fetch_payloads(source_url=None, local_file=None):
    """
    Fetch payloads (e.g., XSS or RCE payloads) from a remote URL or a local file.
    :param source_url: URL to fetch payloads from (e.g., a GitHub repository)
    :param local_file: Path to a local file containing payloads
    :return: List of payloads
    """
    payloads = []
    try:
        if source_url:
            # Fetch payloads from a remote URL
            response = requests.get(source_url)
            if response.status_code == 200:
                payloads = response.text.splitlines()  # Split the response into individual payloads
        elif local_file:
            # Fetch payloads from a local file
            with open(local_file, 'r') as file:
                payloads = file.read().splitlines()  # Read the file and split into payloads
    except Exception as e:
        print(f"Error fetching payloads: {e}")  # Handle exceptions during payload fetching
    return payloads

# Fetch XSS payloads from a remote repository (PayloadsAllTheThings)
xss_payloads = fetch_payloads(source_url='https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/XSS%20Injection/Intruder/xss-payloads.txt')

# Fetch RCE payloads from a remote repository (PayloadsAllTheThings)
rce_payloads = fetch_payloads(source_url='https://raw.githubusercontent.com/swisskyrepo/PayloadsAllTheThings/master/Command%20Injection/Intruder/command-injection.txt')

def scan_website(target_domain):
    """
    Scan a website for vulnerabilities such as XSS, SQLi, and RCE.
    :param target_domain: The domain or URL of the website to scan
    :return: Dictionary containing detected vulnerabilities
    """
    vulnerabilities = defaultdict(list)  # Dictionary to store detected vulnerabilities
    try:
        # Fetch the website's HTML content
        response = requests.get(target_domain)
        soup = BeautifulSoup(response.content, 'html.parser')  # Parse the HTML content

        # Check for vulnerabilities in JavaScript libraries
        scripts = soup.find_all('script')  # Find all <script> tags
        for script in scripts:
            src = script.get('src')  # Get the 'src' attribute of the script
            if src:
                src_url = urljoin(target_domain, src)  # Construct the full URL of the script
                parsed_url = urlparse(src_url)  # Parse the URL
                if parsed_url.netloc:
                    # Extract the library name from the URL
                    lib_name = os.path.basename(parsed_url.path)
                    if lib_name in cve_db:
                        # Add vulnerabilities associated with the library
                        for cve in cve_db[lib_name]:
                            vulnerabilities[cve['id']].append({
                                'library': lib_name,
                                'version': cve['version'],
                                'description': cve['summary']
                            })

        # Check for SQL injection vulnerabilities
        forms = soup.find_all('form')  # Find all <form> tags
        for form in forms:
            action = form.get('action')  # Get the 'action' attribute of the form
            if action:
                action_url = urljoin(target_domain, action)  # Construct the full URL of the form action
                test_url = action_url + "?test=' OR '1'='1"  # Test for SQL injection
                response = requests.get(test_url)
                if 'error' in response.text.lower():  # Check if the response indicates an SQL error
                    vulnerabilities['SQLi'].append({
                        'url': action_url,
                        'parameter': 'test',
                        'description': 'SQL injection vulnerability detected'
                    })

        # Check for cross-site scripting (XSS) vulnerabilities
        inputs = soup.find_all('input')  # Find all <input> tags
        for input in inputs:
            name = input.get('name')  # Get the 'name' attribute of the input
            if name:
                for payload in xss_payloads:
                    # Test for XSS by injecting payloads
                    response = requests.get(target_domain, params={name: payload})
                    if payload in response.text:  # Check if the payload is reflected in the response
                        vulnerabilities['XSS'].append({
                            'url': target_domain,
                            'parameter': name,
                            'description': 'Cross-site scripting vulnerability detected'
                        })
                        break

        # Check for Remote Command Execution (RCE) vulnerabilities
        for form in forms:
            action = form.get('action')
            if action:
                action_url = urljoin(target_domain, action)
                for payload in rce_payloads:
                    # Test for RCE by injecting payloads
                    test_url = action_url + "?cmd=" + payload
                    response = requests.get(test_url)
                    if 'uid=' in response.text or 'root:' in response.text:  # Check for signs of command execution
                        vulnerabilities['RCE'].append({
                            'url': action_url,
                            'parameter': 'cmd',
                            'description': 'Remote Command Execution vulnerability detected'
                        })
                        break
    except requests.exceptions.RequestException as e:
        print(f"Error scanning {target_domain}: {e}")  # Handle exceptions during scanning
    return vulnerabilities

def generate_report(vulnerabilities, target_domain):
    """
    Generate a Word document report for the detected vulnerabilities.
    :param vulnerabilities: Dictionary containing detected vulnerabilities
    :param target_domain: The domain or URL of the scanned website
    """
    doc = Document()  # Create a new Word document
    doc.add_heading(f'Vulnerability Report for {target_domain}', 0)  # Add a title to the document

    # Add details of each vulnerability to the document
    for vuln_id, vuln_list in vulnerabilities.items():
        doc.add_heading(vuln_id, level=1)  # Add a heading for the vulnerability type
        for vuln in vuln_list:
            if 'library' in vuln:
                # Add details for library-related vulnerabilities
                doc.add_paragraph(f"{vuln['description']} ({vuln['library']} {vuln['version']})")
            else:
                # Add details for other vulnerabilities
                doc.add_paragraph(f"{vuln['description']} (URL: {vuln['url']}, Parameter: {vuln['parameter']})")

    # Save the document to the Downloads directory
    download_dir = os.path.join(os.path.expanduser('~'), 'Downloads')
    if not os.path.exists(download_dir):
        os.makedirs(download_dir)  # Create the Downloads directory if it doesn't exist
    report_path = os.path.join(download_dir, f'{target_domain.replace("http://", "").replace("https://", "").replace("/", "_")}_report.docx')
    doc.save(report_path)  # Save the document
    print("Report saved to", report_path)

# Main execution
if __name__ == "__main__":
    target_domain = input("Enter the domain you want to target: ")  # Prompt the user for the target domain
    vulnerabilities = scan_website(target_domain)  # Scan the website for vulnerabilities
    generate_report(vulnerabilities, target_domain)  # Generate a report
    print("Report saved to the Downloads directory")
